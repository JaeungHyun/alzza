{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_steel_dataset():\n",
    "    ## pandas로 불러오는 것 적용하기\n",
    "    with open('data/chap03/faults.csv') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader, None)\n",
    "        rows = []\n",
    "        for row in csvreader:\n",
    "            rows.append(row)\n",
    "\n",
    "    input_cnt, output_cnt = 27, 7\n",
    "    data = np.asarray(rows, dtype='float32')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_steel_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1941, 34)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>270900.0</td>\n",
       "      <td>270944.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>24220.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8182</td>\n",
       "      <td>-0.2913</td>\n",
       "      <td>0.5822</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>645.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>2538079.0</td>\n",
       "      <td>2538108.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11397.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7931</td>\n",
       "      <td>-0.1756</td>\n",
       "      <td>0.2984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>829.0</td>\n",
       "      <td>835.0</td>\n",
       "      <td>1553913.0</td>\n",
       "      <td>1553931.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7972.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>-0.1228</td>\n",
       "      <td>0.2150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853.0</td>\n",
       "      <td>860.0</td>\n",
       "      <td>369370.0</td>\n",
       "      <td>369415.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>18996.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8444</td>\n",
       "      <td>-0.1568</td>\n",
       "      <td>0.5212</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1289.0</td>\n",
       "      <td>1306.0</td>\n",
       "      <td>498078.0</td>\n",
       "      <td>498335.0</td>\n",
       "      <td>2409.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>246930.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9338</td>\n",
       "      <td>-0.1992</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>249.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>325780.0</td>\n",
       "      <td>325796.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>35033.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4286</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.7254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>144.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>340581.0</td>\n",
       "      <td>340598.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>34599.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4516</td>\n",
       "      <td>-0.0582</td>\n",
       "      <td>0.8173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>145.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>386779.0</td>\n",
       "      <td>386794.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>37572.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4828</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.7079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>137.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>422497.0</td>\n",
       "      <td>422528.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>52715.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0606</td>\n",
       "      <td>-0.0171</td>\n",
       "      <td>0.9919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>1261.0</td>\n",
       "      <td>1281.0</td>\n",
       "      <td>87951.0</td>\n",
       "      <td>87967.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11682.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2000</td>\n",
       "      <td>-0.1139</td>\n",
       "      <td>0.5296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1941 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0       1          2          3       4     5      6         7   \\\n",
       "0       42.0    50.0   270900.0   270944.0   267.0  17.0   44.0   24220.0   \n",
       "1      645.0   651.0  2538079.0  2538108.0   108.0  10.0   30.0   11397.0   \n",
       "2      829.0   835.0  1553913.0  1553931.0    71.0   8.0   19.0    7972.0   \n",
       "3      853.0   860.0   369370.0   369415.0   176.0  13.0   45.0   18996.0   \n",
       "4     1289.0  1306.0   498078.0   498335.0  2409.0  60.0  260.0  246930.0   \n",
       "...      ...     ...        ...        ...     ...   ...    ...       ...   \n",
       "1936   249.0   277.0   325780.0   325796.0   273.0  54.0   22.0   35033.0   \n",
       "1937   144.0   175.0   340581.0   340598.0   287.0  44.0   24.0   34599.0   \n",
       "1938   145.0   174.0   386779.0   386794.0   292.0  40.0   22.0   37572.0   \n",
       "1939   137.0   170.0   422497.0   422528.0   419.0  97.0   47.0   52715.0   \n",
       "1940  1261.0  1281.0    87951.0    87967.0   103.0  26.0   22.0   11682.0   \n",
       "\n",
       "         8      9   ...      24      25      26   27   28   29   30   31   32  \\\n",
       "0      76.0  108.0  ...  0.8182 -0.2913  0.5822  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1      84.0  123.0  ...  0.7931 -0.1756  0.2984  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2      99.0  125.0  ...  0.6667 -0.1228  0.2150  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3      99.0  126.0  ...  0.8444 -0.1568  0.5212  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4      37.0  126.0  ...  0.9338 -0.1992  1.0000  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...     ...    ...  ...     ...     ...     ...  ...  ...  ...  ...  ...  ...   \n",
       "1936  119.0  141.0  ... -0.4286  0.0026  0.7254  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1937  112.0  133.0  ... -0.4516 -0.0582  0.8173  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1938  120.0  140.0  ... -0.4828  0.0052  0.7079  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1939  117.0  140.0  ... -0.0606 -0.0171  0.9919  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1940  101.0  133.0  ... -0.2000 -0.1139  0.5296  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "       33  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  \n",
       "...   ...  \n",
       "1936  1.0  \n",
       "1937  1.0  \n",
       "1938  1.0  \n",
       "1939  1.0  \n",
       "1940  1.0  \n",
       "\n",
       "[1941 rows x 34 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = data[:,:27]\n",
    "yTrain = data[:,27:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain = yTrain.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1941, 27)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xTrain, yTrain, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "train_loader = DataLoader(trn, batch_size = 10, shuffle=True)\n",
    "\n",
    "trn = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "test_loader = DataLoader(trn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.linear_1 = torch.nn.Linear(self.input_size, self.hidden_size, bias=True)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        linear1 = self.linear_1(input_tensor)\n",
    "        #out = torch.sigmoid(linear1)\n",
    "        \n",
    "        return linear1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"Training on {}\".format(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model        = NeuralNet(27, 7).to(DEVICE)\n",
    "optimizer    = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "#criterion    = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (linear_1): Linear(in_features=27, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        _, target = target.max(dim=1)\n",
    "        loss = F.cross_entropy(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "# ## 테스트하기\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE, dtype=torch.long)\n",
    "            output = model(data)\n",
    "\n",
    "            # 배치 오차를 합산\n",
    "            _, target = target.max(dim=1)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "\n",
    "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1552 (0%)]\tLoss: 146097.609375\n",
      "Train Epoch: 1 [100/1552 (6%)]\tLoss: 145713709056.000000\n",
      "Train Epoch: 1 [200/1552 (13%)]\tLoss: 14911761408.000000\n",
      "Train Epoch: 1 [300/1552 (19%)]\tLoss: 25208051712.000000\n",
      "Train Epoch: 1 [400/1552 (26%)]\tLoss: 56252047360.000000\n",
      "Train Epoch: 1 [500/1552 (32%)]\tLoss: 53557891072.000000\n",
      "Train Epoch: 1 [600/1552 (38%)]\tLoss: 12163872768.000000\n",
      "Train Epoch: 1 [700/1552 (45%)]\tLoss: 26824716288.000000\n",
      "Train Epoch: 1 [800/1552 (51%)]\tLoss: 23201972224.000000\n",
      "Train Epoch: 1 [900/1552 (58%)]\tLoss: 19947540480.000000\n",
      "Train Epoch: 1 [1000/1552 (64%)]\tLoss: 19670274048.000000\n",
      "Train Epoch: 1 [1100/1552 (71%)]\tLoss: 36833042432.000000\n",
      "Train Epoch: 1 [1200/1552 (77%)]\tLoss: 12375590912.000000\n",
      "Train Epoch: 1 [1300/1552 (83%)]\tLoss: 38398189568.000000\n",
      "Train Epoch: 1 [1400/1552 (90%)]\tLoss: 8782663680.000000\n",
      "Train Epoch: 1 [1500/1552 (96%)]\tLoss: 25201289216.000000\n",
      "[1] Test Loss: 26159397093.2648, Accuracy: 28.53%\n",
      "\n",
      "Train Epoch: 2 [0/1552 (0%)]\tLoss: 12204156928.000000\n",
      "Train Epoch: 2 [100/1552 (6%)]\tLoss: 11137832960.000000\n",
      "Train Epoch: 2 [200/1552 (13%)]\tLoss: 14497802240.000000\n",
      "Train Epoch: 2 [300/1552 (19%)]\tLoss: 7796421632.000000\n",
      "Train Epoch: 2 [400/1552 (26%)]\tLoss: 42409140224.000000\n",
      "Train Epoch: 2 [500/1552 (32%)]\tLoss: 15095886848.000000\n",
      "Train Epoch: 2 [600/1552 (38%)]\tLoss: 44143927296.000000\n",
      "Train Epoch: 2 [700/1552 (45%)]\tLoss: 36049682432.000000\n",
      "Train Epoch: 2 [800/1552 (51%)]\tLoss: 30363488256.000000\n",
      "Train Epoch: 2 [900/1552 (58%)]\tLoss: 9907951616.000000\n",
      "Train Epoch: 2 [1000/1552 (64%)]\tLoss: 38426595328.000000\n",
      "Train Epoch: 2 [1100/1552 (71%)]\tLoss: 44100952064.000000\n",
      "Train Epoch: 2 [1200/1552 (77%)]\tLoss: 34889400320.000000\n",
      "Train Epoch: 2 [1300/1552 (83%)]\tLoss: 16669395968.000000\n",
      "Train Epoch: 2 [1400/1552 (90%)]\tLoss: 10251855872.000000\n",
      "Train Epoch: 2 [1500/1552 (96%)]\tLoss: 51487252480.000000\n",
      "[2] Test Loss: 7000731182.8895, Accuracy: 47.56%\n",
      "\n",
      "Train Epoch: 3 [0/1552 (0%)]\tLoss: 6714352640.000000\n",
      "Train Epoch: 3 [100/1552 (6%)]\tLoss: 77098860544.000000\n",
      "Train Epoch: 3 [200/1552 (13%)]\tLoss: 12324206592.000000\n",
      "Train Epoch: 3 [300/1552 (19%)]\tLoss: 16538118144.000000\n",
      "Train Epoch: 3 [400/1552 (26%)]\tLoss: 16583073792.000000\n",
      "Train Epoch: 3 [500/1552 (32%)]\tLoss: 18239070208.000000\n",
      "Train Epoch: 3 [600/1552 (38%)]\tLoss: 23835400192.000000\n",
      "Train Epoch: 3 [700/1552 (45%)]\tLoss: 46146797568.000000\n",
      "Train Epoch: 3 [800/1552 (51%)]\tLoss: 33941530624.000000\n",
      "Train Epoch: 3 [900/1552 (58%)]\tLoss: 46934216704.000000\n",
      "Train Epoch: 3 [1000/1552 (64%)]\tLoss: 35952857088.000000\n",
      "Train Epoch: 3 [1100/1552 (71%)]\tLoss: 4758530560.000000\n",
      "Train Epoch: 3 [1200/1552 (77%)]\tLoss: 58864074752.000000\n",
      "Train Epoch: 3 [1300/1552 (83%)]\tLoss: 20841631744.000000\n",
      "Train Epoch: 3 [1400/1552 (90%)]\tLoss: 2681436928.000000\n",
      "Train Epoch: 3 [1500/1552 (96%)]\tLoss: 24317941760.000000\n",
      "[3] Test Loss: 17470234336.0000, Accuracy: 32.39%\n",
      "\n",
      "Train Epoch: 4 [0/1552 (0%)]\tLoss: 37343989760.000000\n",
      "Train Epoch: 4 [100/1552 (6%)]\tLoss: 15924590592.000000\n",
      "Train Epoch: 4 [200/1552 (13%)]\tLoss: 4537206272.000000\n",
      "Train Epoch: 4 [300/1552 (19%)]\tLoss: 10726381568.000000\n",
      "Train Epoch: 4 [400/1552 (26%)]\tLoss: 8134396416.000000\n",
      "Train Epoch: 4 [500/1552 (32%)]\tLoss: 14108663808.000000\n",
      "Train Epoch: 4 [600/1552 (38%)]\tLoss: 23122511872.000000\n",
      "Train Epoch: 4 [700/1552 (45%)]\tLoss: 13401875456.000000\n",
      "Train Epoch: 4 [800/1552 (51%)]\tLoss: 26109757440.000000\n",
      "Train Epoch: 4 [900/1552 (58%)]\tLoss: 6114479104.000000\n",
      "Train Epoch: 4 [1000/1552 (64%)]\tLoss: 7536945664.000000\n",
      "Train Epoch: 4 [1100/1552 (71%)]\tLoss: 44267732992.000000\n",
      "Train Epoch: 4 [1200/1552 (77%)]\tLoss: 3078554624.000000\n",
      "Train Epoch: 4 [1300/1552 (83%)]\tLoss: 35482775552.000000\n",
      "Train Epoch: 4 [1400/1552 (90%)]\tLoss: 17228009472.000000\n",
      "Train Epoch: 4 [1500/1552 (96%)]\tLoss: 36811161600.000000\n",
      "[4] Test Loss: 14298257074.0154, Accuracy: 43.44%\n",
      "\n",
      "Train Epoch: 5 [0/1552 (0%)]\tLoss: 4704972800.000000\n",
      "Train Epoch: 5 [100/1552 (6%)]\tLoss: 32347156480.000000\n",
      "Train Epoch: 5 [200/1552 (13%)]\tLoss: 16144721920.000000\n",
      "Train Epoch: 5 [300/1552 (19%)]\tLoss: 14424443904.000000\n",
      "Train Epoch: 5 [400/1552 (26%)]\tLoss: 17870833664.000000\n",
      "Train Epoch: 5 [500/1552 (32%)]\tLoss: 31594293248.000000\n",
      "Train Epoch: 5 [600/1552 (38%)]\tLoss: 16618222592.000000\n",
      "Train Epoch: 5 [700/1552 (45%)]\tLoss: 31875203072.000000\n",
      "Train Epoch: 5 [800/1552 (51%)]\tLoss: 13041108992.000000\n",
      "Train Epoch: 5 [900/1552 (58%)]\tLoss: 20793014272.000000\n",
      "Train Epoch: 5 [1000/1552 (64%)]\tLoss: 5989094912.000000\n",
      "Train Epoch: 5 [1100/1552 (71%)]\tLoss: 3752824064.000000\n",
      "Train Epoch: 5 [1200/1552 (77%)]\tLoss: 10758690816.000000\n",
      "Train Epoch: 5 [1300/1552 (83%)]\tLoss: 28561883136.000000\n",
      "Train Epoch: 5 [1400/1552 (90%)]\tLoss: 12984076288.000000\n",
      "Train Epoch: 5 [1500/1552 (96%)]\tLoss: 25617795072.000000\n",
      "[5] Test Loss: 94390443139.2905, Accuracy: 21.34%\n",
      "\n",
      "Train Epoch: 6 [0/1552 (0%)]\tLoss: 109461504000.000000\n",
      "Train Epoch: 6 [100/1552 (6%)]\tLoss: 19922421760.000000\n",
      "Train Epoch: 6 [200/1552 (13%)]\tLoss: 12095113216.000000\n",
      "Train Epoch: 6 [300/1552 (19%)]\tLoss: 4888899072.000000\n",
      "Train Epoch: 6 [400/1552 (26%)]\tLoss: 25626972160.000000\n",
      "Train Epoch: 6 [500/1552 (32%)]\tLoss: 19300388864.000000\n",
      "Train Epoch: 6 [600/1552 (38%)]\tLoss: 23499501568.000000\n",
      "Train Epoch: 6 [700/1552 (45%)]\tLoss: 9197910016.000000\n",
      "Train Epoch: 6 [800/1552 (51%)]\tLoss: 9144456192.000000\n",
      "Train Epoch: 6 [900/1552 (58%)]\tLoss: 8290634752.000000\n",
      "Train Epoch: 6 [1000/1552 (64%)]\tLoss: 25202864128.000000\n",
      "Train Epoch: 6 [1100/1552 (71%)]\tLoss: 20917544960.000000\n",
      "Train Epoch: 6 [1200/1552 (77%)]\tLoss: 8011475968.000000\n",
      "Train Epoch: 6 [1300/1552 (83%)]\tLoss: 27111813120.000000\n",
      "Train Epoch: 6 [1400/1552 (90%)]\tLoss: 30244098048.000000\n",
      "Train Epoch: 6 [1500/1552 (96%)]\tLoss: 64395804672.000000\n",
      "[6] Test Loss: 11716094569.9126, Accuracy: 33.16%\n",
      "\n",
      "Train Epoch: 7 [0/1552 (0%)]\tLoss: 16096223232.000000\n",
      "Train Epoch: 7 [100/1552 (6%)]\tLoss: 11476801536.000000\n",
      "Train Epoch: 7 [200/1552 (13%)]\tLoss: 49515196416.000000\n",
      "Train Epoch: 7 [300/1552 (19%)]\tLoss: 23800352768.000000\n",
      "Train Epoch: 7 [400/1552 (26%)]\tLoss: 6232513024.000000\n",
      "Train Epoch: 7 [500/1552 (32%)]\tLoss: 10651223040.000000\n",
      "Train Epoch: 7 [600/1552 (38%)]\tLoss: 8356086784.000000\n",
      "Train Epoch: 7 [700/1552 (45%)]\tLoss: 15687145472.000000\n",
      "Train Epoch: 7 [800/1552 (51%)]\tLoss: 17305630720.000000\n",
      "Train Epoch: 7 [900/1552 (58%)]\tLoss: 31365005312.000000\n",
      "Train Epoch: 7 [1000/1552 (64%)]\tLoss: 58799529984.000000\n",
      "Train Epoch: 7 [1100/1552 (71%)]\tLoss: 12272404480.000000\n",
      "Train Epoch: 7 [1200/1552 (77%)]\tLoss: 28759267328.000000\n",
      "Train Epoch: 7 [1300/1552 (83%)]\tLoss: 65252581376.000000\n",
      "Train Epoch: 7 [1400/1552 (90%)]\tLoss: 9544679424.000000\n",
      "Train Epoch: 7 [1500/1552 (96%)]\tLoss: 1344697088.000000\n",
      "[7] Test Loss: 16596143263.1774, Accuracy: 45.50%\n",
      "\n",
      "Train Epoch: 8 [0/1552 (0%)]\tLoss: 14512460800.000000\n",
      "Train Epoch: 8 [100/1552 (6%)]\tLoss: 18020679680.000000\n",
      "Train Epoch: 8 [200/1552 (13%)]\tLoss: 11453881344.000000\n",
      "Train Epoch: 8 [300/1552 (19%)]\tLoss: 6809771008.000000\n",
      "Train Epoch: 8 [400/1552 (26%)]\tLoss: 13862529024.000000\n",
      "Train Epoch: 8 [500/1552 (32%)]\tLoss: 19795251200.000000\n",
      "Train Epoch: 8 [600/1552 (38%)]\tLoss: 19236581376.000000\n",
      "Train Epoch: 8 [700/1552 (45%)]\tLoss: 7879251456.000000\n",
      "Train Epoch: 8 [800/1552 (51%)]\tLoss: 2157626112.000000\n",
      "Train Epoch: 8 [900/1552 (58%)]\tLoss: 28853440512.000000\n",
      "Train Epoch: 8 [1000/1552 (64%)]\tLoss: 3755141632.000000\n",
      "Train Epoch: 8 [1100/1552 (71%)]\tLoss: 6897533952.000000\n",
      "Train Epoch: 8 [1200/1552 (77%)]\tLoss: 34255923200.000000\n",
      "Train Epoch: 8 [1300/1552 (83%)]\tLoss: 32350679040.000000\n",
      "Train Epoch: 8 [1400/1552 (90%)]\tLoss: 26425659392.000000\n",
      "Train Epoch: 8 [1500/1552 (96%)]\tLoss: 3566213888.000000\n",
      "[8] Test Loss: 34458246829.5733, Accuracy: 17.74%\n",
      "\n",
      "Train Epoch: 9 [0/1552 (0%)]\tLoss: 29210841088.000000\n",
      "Train Epoch: 9 [100/1552 (6%)]\tLoss: 32930136064.000000\n",
      "Train Epoch: 9 [200/1552 (13%)]\tLoss: 31408787456.000000\n",
      "Train Epoch: 9 [300/1552 (19%)]\tLoss: 12495552512.000000\n",
      "Train Epoch: 9 [400/1552 (26%)]\tLoss: 38519480320.000000\n",
      "Train Epoch: 9 [500/1552 (32%)]\tLoss: 60447154176.000000\n",
      "Train Epoch: 9 [600/1552 (38%)]\tLoss: 14115751936.000000\n",
      "Train Epoch: 9 [700/1552 (45%)]\tLoss: 26674974720.000000\n",
      "Train Epoch: 9 [800/1552 (51%)]\tLoss: 30249621504.000000\n",
      "Train Epoch: 9 [900/1552 (58%)]\tLoss: 45967171584.000000\n",
      "Train Epoch: 9 [1000/1552 (64%)]\tLoss: 12604474368.000000\n",
      "Train Epoch: 9 [1100/1552 (71%)]\tLoss: 22737360896.000000\n",
      "Train Epoch: 9 [1200/1552 (77%)]\tLoss: 5832228864.000000\n",
      "Train Epoch: 9 [1300/1552 (83%)]\tLoss: 20658434048.000000\n",
      "Train Epoch: 9 [1400/1552 (90%)]\tLoss: 2943816192.000000\n",
      "Train Epoch: 9 [1500/1552 (96%)]\tLoss: 19965825024.000000\n",
      "[9] Test Loss: 218118007495.4036, Accuracy: 11.05%\n",
      "\n",
      "Train Epoch: 10 [0/1552 (0%)]\tLoss: 208459481088.000000\n",
      "Train Epoch: 10 [100/1552 (6%)]\tLoss: 12610391040.000000\n",
      "Train Epoch: 10 [200/1552 (13%)]\tLoss: 38868414464.000000\n",
      "Train Epoch: 10 [300/1552 (19%)]\tLoss: 14878157824.000000\n",
      "Train Epoch: 10 [400/1552 (26%)]\tLoss: 36050575360.000000\n",
      "Train Epoch: 10 [500/1552 (32%)]\tLoss: 5143523328.000000\n",
      "Train Epoch: 10 [600/1552 (38%)]\tLoss: 32189392896.000000\n",
      "Train Epoch: 10 [700/1552 (45%)]\tLoss: 25216839680.000000\n",
      "Train Epoch: 10 [800/1552 (51%)]\tLoss: 42084704256.000000\n",
      "Train Epoch: 10 [900/1552 (58%)]\tLoss: 27468591104.000000\n",
      "Train Epoch: 10 [1000/1552 (64%)]\tLoss: 28947709952.000000\n",
      "Train Epoch: 10 [1100/1552 (71%)]\tLoss: 10849097728.000000\n",
      "Train Epoch: 10 [1200/1552 (77%)]\tLoss: 3859845120.000000\n",
      "Train Epoch: 10 [1300/1552 (83%)]\tLoss: 11922686976.000000\n",
      "Train Epoch: 10 [1400/1552 (90%)]\tLoss: 43704999936.000000\n",
      "Train Epoch: 10 [1500/1552 (96%)]\tLoss: 15670152192.000000\n",
      "[10] Test Loss: 48090817459.8252, Accuracy: 18.25%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(\n",
    "          epoch, test_loss, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
