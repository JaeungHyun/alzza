{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../chap10/rnn_basic_model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLstmModel(RnnBasicModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_alloc_lstm_layer(self, input_shape, hconfig):\n",
    "    inseq = get_conf_param(hconfig, 'inseq', True)\n",
    "    outseq = get_conf_param(hconfig, 'outseq', True)\n",
    "    use_state = get_conf_param(hconfig, 'use_state', False)\n",
    "\n",
    "    if inseq:\n",
    "        timesteps1, timefeats = input_shape\n",
    "    else:\n",
    "        timesteps1 = get_conf_param(hconfig, 'timesteps') + 1\n",
    "        timefeats = np.prod(input_shape)\n",
    "    \n",
    "    recur_size = get_conf_param(hconfig, 'recur_size')\n",
    "\n",
    "    ex_inp_dim = timefeats + recur_size\n",
    "    weight, bias = self.alloc_param_pair([ex_inp_dim, 4*recur_size])\n",
    "    bias[0*recur_size:1*recur_size] = 1.0\n",
    "\n",
    "    if outseq:\n",
    "        output_shape = [timesteps1, recur_size]\n",
    "    else:\n",
    "        output_shape = [recur_size]\n",
    "    \n",
    "    rnn_info = [inseq, outseq, timesteps1, timefeats, recur_size, use_state]\n",
    "    \n",
    "    return {'w':weight, 'b':bias, 'info':rnn_info}, output_shape\n",
    "\n",
    "RnnLstmModel.alloc_lstm_layer = rnn_alloc_lstm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward_lstm_layer(self, x, hconfig, pm):\n",
    "    inseq, outseq, timesteps1, timefeats, recur_size, use_state = pm['info']\n",
    "    mb_size = x.shape[0]\n",
    "\n",
    "    if inseq:\n",
    "        x_slices = x[:, 1:, :].transpose([1,0,2])\n",
    "        lengths = x[:, 0, 0].astype(np.int32)\n",
    "        max_length = np.max(lengths)\n",
    "    else:\n",
    "        x_slice = x\n",
    "        max_length = timesteps1 - 1\n",
    "        lengths = [max_length] * mb_size\n",
    "\n",
    "    recurrent = np.zeros([mb_size, recur_size])\n",
    "    state = np.zeros([mb_size, recur_size])\n",
    "    outputs, aux_steps = [], []\n",
    "    \n",
    "    for n in range(max_length):\n",
    "        if inseq: x_slice = x_slices[n]\n",
    "            \n",
    "        ex_inp = np.hstack([x_slice, recurrent])\n",
    "        affine = np.matmul(ex_inp, pm['w']) + pm['b']\n",
    "\n",
    "        forget_gate = sigmoid(affine[:, 0*recur_size:1*recur_size])\n",
    "        input_gate  = sigmoid(affine[:, 1*recur_size:2*recur_size])\n",
    "        output_gate = sigmoid(affine[:, 2*recur_size:3*recur_size])\n",
    "        block_input = tanh   (affine[:, 3*recur_size:4*recur_size])\n",
    "\n",
    "        state_tmp = state\n",
    "        state = state_tmp * forget_gate + block_input * input_gate\n",
    "        \n",
    "        recur_tmp = tanh(state)\n",
    "        recurrent = recur_tmp * output_gate\n",
    "\n",
    "        if use_state: outputs.append(state)\n",
    "        else: outputs.append(recurrent)\n",
    "\n",
    "        aux_step = [ex_inp, state_tmp, block_input, input_gate, \\\n",
    "                    forget_gate, output_gate, recur_tmp]\n",
    "        aux_steps.append(aux_step)\n",
    "        \n",
    "    if outseq:\n",
    "        output = np.zeros([mb_size, timesteps1, recur_size])\n",
    "        output[:, 0, 0] = lengths\n",
    "        output[:, 1:, :] = np.asarray(outputs).transpose([1, 0, 2])\n",
    "    else:\n",
    "        output = np.zeros([mb_size, recur_size])\n",
    "        for n in range(mb_size):\n",
    "            output[n] = outputs[lengths[n]-1][n]\n",
    "        \n",
    "    return output, [x, lengths, max_length, outputs, aux_steps]\n",
    "\n",
    "RnnLstmModel.forward_lstm_layer = rnn_forward_lstm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backprop_lstm_layer(self, G_y, hconfig, pm, aux):\n",
    "    inseq, outseq, timesteps1, timefeats, recur_size, use_state = pm['info']\n",
    "    x, lengths, max_length, outputs, aux_steps = aux\n",
    "    mb_size = x.shape[0]\n",
    "\n",
    "    G_weight = np.zeros_like(pm['w'])\n",
    "    G_bias = np.zeros_like(pm['b'])\n",
    "    G_x = np.zeros(x.shape)\n",
    "    G_recurrent = np.zeros([mb_size, recur_size])\n",
    "    G_state = np.zeros([mb_size, recur_size])\n",
    "\n",
    "    if inseq: G_x[:, 0, 0] = lengths\n",
    "\n",
    "    if outseq:\n",
    "        G_outputs = G_y[:, 1:, :].transpose([1, 0, 2])\n",
    "    else:\n",
    "        G_outputs = np.zeros([max_length, mb_size, recur_size])\n",
    "        for n in range(mb_size):\n",
    "            G_outputs[lengths[n]-1, n, :] = G_y[n]\n",
    "\n",
    "    for n in reversed(range(0, max_length)):\n",
    "        if use_state: G_state += G_outputs[n]\n",
    "        else: G_recurrent += G_outputs[n]\n",
    "\n",
    "        ex_inp, state_tmp, block_input, input_gate, forget_gate, \\\n",
    "                              output_gate, recur_tmp = aux_steps[n]\n",
    "\n",
    "        G_recur_tmp = G_recurrent * output_gate\n",
    "        G_output_gate = G_recurrent * recur_tmp\n",
    "\n",
    "        G_state += tanh_derv(recur_tmp) * G_recur_tmp\n",
    "        \n",
    "        G_input_gate = G_state * block_input\n",
    "        G_block_input = G_state * input_gate\n",
    "        \n",
    "        G_forget_gate = G_state * state_tmp\n",
    "        G_state = G_state * forget_gate\n",
    "        \n",
    "        G_affine = np.zeros([mb_size, 4*recur_size])\n",
    "\n",
    "\n",
    "        G_affine[:, 0*recur_size:1*recur_size] = \\\n",
    "                                sigmoid_derv(forget_gate) * G_forget_gate\n",
    "        G_affine[:, 1*recur_size:2*recur_size] = \\\n",
    "                                sigmoid_derv(input_gate)  * G_input_gate\n",
    "        G_affine[:, 2*recur_size:3*recur_size] = \\\n",
    "                                sigmoid_derv(output_gate) * G_output_gate\n",
    "        G_affine[:, 3*recur_size:4*recur_size] = \\\n",
    "                                tanh_derv   (block_input) * G_block_input\n",
    "        \n",
    "        g_affine_weight = ex_inp.transpose()\n",
    "        g_affine_input = pm['w'].transpose()\n",
    "\n",
    "        G_weight += np.matmul(g_affine_weight, G_affine)\n",
    "        G_bias += np.sum(G_affine, axis=0)\n",
    "        G_ex_inp = np.matmul(G_affine, g_affine_input)\n",
    "        \n",
    "        if inseq: G_x[:,n+1,:] = G_ex_inp[:, :timefeats]\n",
    "        else: G_x[:,:] += G_ex_inp[:, :timefeats]\n",
    "            \n",
    "        G_recurrent = G_ex_inp[:, timefeats:]\n",
    "    \n",
    "    self.update_param(pm, 'w', G_weight)\n",
    "    self.update_param(pm, 'b', G_bias)\n",
    "    \n",
    "    return G_x\n",
    "    \n",
    "RnnLstmModel.backprop_lstm_layer = rnn_backprop_lstm_layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
