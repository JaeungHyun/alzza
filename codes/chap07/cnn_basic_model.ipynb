{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../chap06/adam_model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnBasicModel(AdamModel):\n",
    "    def __init__(self, name, dataset, hconfigs, show_maps = False):\n",
    "        if isinstance(hconfigs, list) and \\\n",
    "        not isinstance(hconfigs[0], (list, int)):\n",
    "            hconfigs = [hconfigs]\n",
    "        self.show_maps = show_maps\n",
    "        self.need_maps = False\n",
    "        self.kernels = []\n",
    "        super(CnnBasicModel, self).__init__(name, dataset, hconfigs)\n",
    "        self.use_adam = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_alloc_layer_param(self, input_shape, hconfig):\n",
    "    layer_type = get_layer_type(hconfig)\n",
    "    \n",
    "    m_name = 'alloc_{}_layer'.format(layer_type)\n",
    "    method = getattr(self, m_name)\n",
    "    pm, output_shape = method(input_shape, hconfig)\n",
    "\n",
    "    return pm, output_shape\n",
    "\n",
    "CnnBasicModel.alloc_layer_param = cnn_basic_alloc_layer_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_forward_layer(self, x, hconfig, pm):\n",
    "    layer_type = get_layer_type(hconfig)\n",
    "    \n",
    "    m_name = 'forward_{}_layer'.format(layer_type)\n",
    "    method = getattr(self, m_name)\n",
    "    y, aux = method(x, hconfig, pm)\n",
    "        \n",
    "    return y, aux\n",
    "\n",
    "CnnBasicModel.forward_layer = cnn_basic_forward_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_backprop_layer(self, G_y, hconfig, pm, aux):\n",
    "    layer_type = get_layer_type(hconfig)\n",
    "    \n",
    "    m_name = 'backprop_{}_layer'.format(layer_type)\n",
    "    method = getattr(self, m_name)\n",
    "    G_input = method(G_y, hconfig, pm, aux)\n",
    "\n",
    "    return G_input\n",
    "\n",
    "CnnBasicModel.backprop_layer = cnn_basic_backprop_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_alloc_full_layer(self, input_shape, hconfig):\n",
    "    input_cnt = np.prod(input_shape)\n",
    "    output_cnt = get_conf_param(hconfig, 'width', hconfig)\n",
    "\n",
    "    weight = np.random.normal(0, self.rand_std, [input_cnt, output_cnt])\n",
    "    bias = np.zeros([output_cnt])\n",
    "\n",
    "    return {'w':weight, 'b':bias}, [output_cnt]\n",
    "    \n",
    "def cnn_basic_alloc_conv_layer(self, input_shape, hconfig):\n",
    "    assert len(input_shape) == 3\n",
    "    xh, xw, xchn = input_shape\n",
    "    kh, kw = get_conf_param_2d(hconfig, 'ksize')\n",
    "    ychn = get_conf_param(hconfig, 'chn')\n",
    "\n",
    "    kernel = np.random.normal(0, self.rand_std, [kh, kw, xchn, ychn])\n",
    "    bias = np.zeros([ychn])\n",
    "\n",
    "    if self.show_maps: self.kernels.append(kernel)\n",
    "\n",
    "    return {'k':kernel, 'b':bias}, [xh, xw, ychn]\n",
    "    \n",
    "def cnn_basic_alloc_pool_layer(self, input_shape, hconfig):\n",
    "    assert len(input_shape) == 3\n",
    "    xh, xw, xchn = input_shape\n",
    "    sh, sw = get_conf_param_2d(hconfig, 'stride')\n",
    "\n",
    "    assert xh % sh == 0\n",
    "    assert xw % sw == 0\n",
    "\n",
    "    return {}, [xh//sh, xw//sw, xchn]\n",
    "\n",
    "CnnBasicModel.alloc_full_layer = cnn_basic_alloc_full_layer\n",
    "CnnBasicModel.alloc_conv_layer = cnn_basic_alloc_conv_layer\n",
    "CnnBasicModel.alloc_max_layer = cnn_basic_alloc_pool_layer\n",
    "CnnBasicModel.alloc_avg_layer = cnn_basic_alloc_pool_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_type(hconfig):\n",
    "    if not isinstance(hconfig, list): return 'full'\n",
    "    return hconfig[0]\n",
    "\n",
    "def get_conf_param(hconfig, key, defval = None):\n",
    "    if not isinstance(hconfig, list): return defval\n",
    "    if len(hconfig) <= 1: return defval\n",
    "    if not key in hconfig[1]: return defval\n",
    "    return hconfig[1][key]\n",
    "    \n",
    "def get_conf_param_2d(hconfig, key, defval = None):\n",
    "    if len(hconfig) <= 1: return defval\n",
    "    if not key in hconfig[1]: return defval\n",
    "    val = hconfig[1][key]\n",
    "    if isinstance(val, list): return val\n",
    "    return [val, val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_forward_full_layer(self, x, hconfig, pm):\n",
    "    if pm is None: return x, None\n",
    "    \n",
    "    x_org_shape = x.shape\n",
    "    \n",
    "    if len(x.shape) != 2:\n",
    "        mb_size = x.shape[0]\n",
    "        x = x.reshape([mb_size, -1])\n",
    "        \n",
    "    affine = np.matmul(x, pm['w']) + pm['b']\n",
    "    y = self.activate(affine, hconfig)\n",
    "    \n",
    "    return y, [x, y, x_org_shape]\n",
    "\n",
    "CnnBasicModel.forward_full_layer = cnn_basic_forward_full_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_backprop_full_layer(self, G_y, hconfig, pm, aux):\n",
    "    if pm is None: return G_y\n",
    "\n",
    "    x, y, x_org_shape = aux\n",
    "    \n",
    "    G_affine = self.activate_derv(G_y, y, hconfig)\n",
    "    \n",
    "    g_affine_weight = x.transpose()\n",
    "    g_affine_input = pm['w'].transpose()\n",
    "    \n",
    "    G_weight = np.matmul(g_affine_weight, G_affine)\n",
    "    G_bias = np.sum(G_affine, axis = 0)\n",
    "    G_input = np.matmul(G_affine, g_affine_input)\n",
    "    \n",
    "    self.update_param(pm, 'w', G_weight)\n",
    "    self.update_param(pm, 'b', G_bias)\n",
    "\n",
    "    return G_input.reshape(x_org_shape)\n",
    "\n",
    "CnnBasicModel.backprop_full_layer = cnn_basic_backprop_full_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_activate(self, affine, hconfig):\n",
    "    if hconfig is None: return affine\n",
    "    \n",
    "    func = get_conf_param(hconfig, 'actfunc', 'relu')\n",
    "    \n",
    "    if func == 'none':      return affine\n",
    "    elif func == 'relu':    return relu(affine)\n",
    "    elif func == 'sigmoid': return sigmoid(affine)\n",
    "    elif func == 'tanh':    return tanh(affine)\n",
    "    else:                   assert 0\n",
    "        \n",
    "def cnn_basic_activate_derv(self, G_y, y, hconfig):\n",
    "    if hconfig is None: return G_y\n",
    "    \n",
    "    func = get_conf_param(hconfig, 'actfunc', 'relu')\n",
    "    \n",
    "    if func == 'none':      return G_y\n",
    "    elif func == 'relu':    return relu_derv(y) * G_y\n",
    "    elif func == 'sigmoid': return sigmoid_derv(y) * G_y\n",
    "    elif func == 'tanh':    return tanh_derv(y) * G_y\n",
    "    else:                   assert 0\n",
    "\n",
    "CnnBasicModel.activate = cnn_basic_activate\n",
    "CnnBasicModel.activate_derv = cnn_basic_activate_derv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_conv_layer_adhoc(self, x, hconfig, pm):\n",
    "    mb_size, xh, xw, xchn = x.shape\n",
    "    kh, kw, _, ychn = pm['k'].shape\n",
    "    \n",
    "    conv = np.zeros((mb_size, xh, xw, ychn))\n",
    "    \n",
    "    for n in range(mb_size):\n",
    "        for r in range(xh):\n",
    "            for c in range(xw):\n",
    "                for ym in range(ychn):\n",
    "                    for i in range(kh):\n",
    "                        for j in range(kw):\n",
    "                            rx = r + i - (kh-1) // 2\n",
    "                            cx = c + j - (kw-1) // 2\n",
    "                            if rx < 0 or rx >= xh: continue\n",
    "                            if cx < 0 or cx >= xw: continue\n",
    "                            for xm in range(xchn):\n",
    "                                kval = pm['k'][i][j][xm][ym]\n",
    "                                ival = x[n][rx][cx][xm]\n",
    "                                conv[n][r][c][ym] += kval * ival\n",
    "\n",
    "    y = self.activate(conv + pm['b'], hconfig)\n",
    "    \n",
    "    return y, [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_conv_layer_better(self, x, hconfig, pm):\n",
    "    mb_size, xh, xw, xchn = x.shape\n",
    "    kh, kw, _, ychn = pm['k'].shape\n",
    "    \n",
    "    conv = np.zeros((mb_size, xh, xw, ychn))\n",
    "\n",
    "    bh, bw = (kh-1)//2, (kw-1)//2\n",
    "    eh, ew = xh + kh - 1, xw + kw - 1\n",
    "    \n",
    "    x_ext = np.zeros((mb_size, eh, ew, xchn))\n",
    "    x_ext[:, bh:bh + xh, bw:bw + xw, :] = x\n",
    "    \n",
    "    k_flat = pm['k'].transpose([3, 0, 1, 2]).reshape([ychn, -1])\n",
    "    \n",
    "    for n in range(mb_size):\n",
    "        for r in range(xh):\n",
    "            for c in range(xw):\n",
    "                for ym in range(ychn):\n",
    "                    xe_flat = x_ext[n, r:r + kh, c:c + kw, :].flatten()\n",
    "                    conv[n, r, c, ym] = (xe_flat*k_flat[ym]).sum()\n",
    "                    \n",
    "    y = self.activate(conv + pm['b'], hconfig)\n",
    "    \n",
    "    return y, [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_forward_conv_layer(self, x, hconfig, pm):\n",
    "    mb_size, xh, xw, xchn = x.shape\n",
    "    kh, kw, _, ychn = pm['k'].shape\n",
    "    \n",
    "    x_flat = get_ext_regions_for_conv(x, kh, kw)\n",
    "    k_flat = pm['k'].reshape([kh*kw*xchn, ychn])\n",
    "    conv_flat = np.matmul(x_flat, k_flat)\n",
    "    conv = conv_flat.reshape([mb_size, xh, xw, ychn])\n",
    "\n",
    "    y = self.activate(conv + pm['b'], hconfig)\n",
    "\n",
    "    if self.need_maps: self.maps.append(y)\n",
    "    \n",
    "    return y, [x_flat, k_flat, x, y]\n",
    "\n",
    "CnnBasicModel.forward_conv_layer = cnn_basic_forward_conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_backprop_conv_layer(self, G_y, hconfig, pm, aux):\n",
    "    x_flat, k_flat, x, y = aux\n",
    "    \n",
    "    kh, kw, xchn, ychn = pm['k'].shape\n",
    "    mb_size, xh, xw, _ = G_y.shape\n",
    "    \n",
    "    G_conv = self.activate_derv(G_y, y, hconfig)\n",
    "\n",
    "    G_conv_flat = G_conv.reshape(mb_size*xh*xw, ychn)\n",
    "\n",
    "    g_conv_k_flat = x_flat.transpose()\n",
    "    g_conv_x_flat = k_flat.transpose()\n",
    "    \n",
    "    G_k_flat = np.matmul(g_conv_k_flat, G_conv_flat)\n",
    "    G_x_flat = np.matmul(G_conv_flat, g_conv_x_flat)\n",
    "    G_bias = np.sum(G_conv_flat, axis = 0)\n",
    "    \n",
    "    G_kernel = G_k_flat.reshape([kh, kw, xchn, ychn])\n",
    "    G_input = undo_ext_regions_for_conv(G_x_flat, x, kh, kw)\n",
    "    \n",
    "    self.update_param(pm, 'k', G_kernel)\n",
    "    self.update_param(pm, 'b', G_bias)\n",
    "    \n",
    "    return G_input\n",
    "\n",
    "CnnBasicModel.backprop_conv_layer = cnn_basic_backprop_conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ext_regions_for_conv(x, kh, kw):\n",
    "    mb_size, xh, xw, xchn = x.shape\n",
    "\n",
    "    regs = get_ext_regions(x, kh, kw, 0)\n",
    "    regs = regs.transpose([2, 0, 1, 3, 4, 5])\n",
    "    \n",
    "    return regs.reshape([mb_size*xh*xw, kh*kw*xchn])\n",
    "\n",
    "def get_ext_regions(x, kh, kw, fill):\n",
    "    mb_size, xh, xw, xchn = x.shape\n",
    "    \n",
    "    eh, ew = xh + kh - 1, xw + kw - 1\n",
    "    bh, bw = (kh-1)//2, (kw-1)//2\n",
    "\n",
    "    x_ext = np.zeros((mb_size, eh, ew, xchn), dtype = 'float32') + fill\n",
    "    x_ext[:, bh:bh + xh, bw:bw + xw, :] = x\n",
    "    \n",
    "    regs = np.zeros((xh, xw, mb_size*kh*kw*xchn), dtype = 'float32')\n",
    "\n",
    "    for r in range(xh):\n",
    "        for c in range(xw):\n",
    "            regs[r, c, :] = x_ext[:, r:r + kh, c:c + kw, :].flatten()\n",
    "\n",
    "    return regs.reshape([xh, xw, mb_size, kh, kw, xchn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undo_ext_regions_for_conv(regs, x, kh, kw):\n",
    "    mb_size, xh, xw, xchn = x.shape\n",
    "\n",
    "    regs = regs.reshape([mb_size, xh, xw, kh, kw, xchn])\n",
    "    regs = regs.transpose([1, 2, 0, 3, 4, 5])\n",
    "    \n",
    "    return undo_ext_regions(regs, kh, kw)\n",
    "\n",
    "def undo_ext_regions(regs, kh, kw):\n",
    "    xh, xw, mb_size, kh, kw, xchn = regs.shape\n",
    "    \n",
    "    eh, ew = xh + kh - 1, xw + kw - 1\n",
    "    bh, bw = (kh-1)//2, (kw-1)//2\n",
    "\n",
    "    gx_ext = np.zeros([mb_size, eh, ew, xchn], dtype = 'float32')\n",
    "\n",
    "    for r in range(xh):\n",
    "        for c in range(xw):\n",
    "            gx_ext[:, r:r + kh, c:c + kw, :] += regs[r, c]\n",
    "\n",
    "    return gx_ext[:, bh:bh + xh, bw:bw + xw, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_forward_avg_layer(self, x, hconfig, pm):\n",
    "    mb_size, xh, xw, chn = x.shape\n",
    "    sh, sw = get_conf_param_2d(hconfig, 'stride')\n",
    "    yh, yw = xh // sh, xw // sw\n",
    "\n",
    "    x1 = x.reshape([mb_size, yh, sh, yw, sw, chn])\n",
    "    x2 = x1.transpose(0, 1, 3, 5, 2, 4)\n",
    "    x3 = x2.reshape([-1, sh*sw])\n",
    "    \n",
    "    y_flat = np.average(x3, 1)\n",
    "    y = y_flat.reshape([mb_size, yh, yw, chn])\n",
    "    \n",
    "    if self.need_maps: self.maps.append(y)\n",
    "\n",
    "    return y, None\n",
    "\n",
    "def cnn_basic_backprop_avg_layer(self, G_y, hconfig, pm, aux):\n",
    "    mb_size, yh, yw, chn = G_y.shape\n",
    "    sh, sw = get_conf_param_2d(hconfig, 'stride')\n",
    "    xh, xw = yh * sh, yw * sw\n",
    "    \n",
    "    gy_flat = G_y.flatten() / (sh * sw)\n",
    "\n",
    "    gx1 = np.zeros([mb_size*yh*yw*chn, sh*sw], dtype = 'float32')\n",
    "    for i in range(sh*sw):\n",
    "        gx1[:, i] = gy_flat\n",
    "    gx2 = gx1.reshape([mb_size, yh, yw, chn, sh, sw])\n",
    "    gx3 = gx2.transpose([0, 1, 4, 2, 5, 3])\n",
    "\n",
    "    G_input = gx3.reshape([mb_size, xh, xw, chn])\n",
    "        \n",
    "    return G_input\n",
    "\n",
    "CnnBasicModel.forward_avg_layer = cnn_basic_forward_avg_layer\n",
    "CnnBasicModel.backprop_avg_layer = cnn_basic_backprop_avg_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_forward_max_layer(self, x, hconfig, pm):\n",
    "    mb_size, xh, xw, chn = x.shape\n",
    "    sh, sw = get_conf_param_2d(hconfig, 'stride')\n",
    "    yh, yw = xh // sh, xw // sw\n",
    "\n",
    "    x1 = x.reshape([mb_size, yh, sh, yw, sw, chn])\n",
    "    x2 = x1.transpose(0, 1, 3, 5, 2, 4)\n",
    "    x3 = x2.reshape([-1, sh*sw])\n",
    "\n",
    "    idxs = np.argmax(x3, axis = 1)\n",
    "    y_flat = x3[np.arange(mb_size*yh*yw*chn), idxs]\n",
    "    y = y_flat.reshape([mb_size, yh, yw, chn])\n",
    "    \n",
    "    if self.need_maps: self.maps.append(y)\n",
    "\n",
    "    return y, idxs\n",
    "\n",
    "def cnn_basic_backprop_max_layer(self, G_y, hconfig, pm, aux):\n",
    "    idxs = aux\n",
    "    \n",
    "    mb_size, yh, yw, chn = G_y.shape\n",
    "    sh, sw = get_conf_param_2d(hconfig, 'stride')\n",
    "    xh, xw = yh * sh, yw * sw\n",
    "    \n",
    "    gy_flat = G_y.flatten()\n",
    "\n",
    "    gx1 = np.zeros([mb_size*yh*yw*chn, sh*sw], dtype = 'float32')\n",
    "    gx1[np.arange(mb_size*yh*yw*chn), idxs] = gy_flat[:]\n",
    "    gx2 = gx1.reshape([mb_size, yh, yw, chn, sh, sw])\n",
    "    gx3 = gx2.transpose([0, 1, 4, 2, 5, 3])\n",
    "\n",
    "    G_input = gx3.reshape([mb_size, xh, xw, chn])\n",
    "        \n",
    "    return G_input\n",
    "\n",
    "CnnBasicModel.forward_max_layer = cnn_basic_forward_max_layer\n",
    "CnnBasicModel.backprop_max_layer = cnn_basic_backprop_max_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_visualize(self, num):\n",
    "    print('Model {} Visualization'.format(self.name))\n",
    "    \n",
    "    self.need_maps = self.show_maps\n",
    "    self.maps = []\n",
    "\n",
    "    deX, deY = self.dataset.get_visualize_data(num)\n",
    "    est = self.get_estimate(deX)\n",
    "\n",
    "    if self.show_maps:\n",
    "        for kernel in self.kernels:\n",
    "            kh, kw, xchn, ychn = kernel.shape\n",
    "            grids = kernel.reshape([kh, kw, -1]).transpose(2, 0, 1)\n",
    "            draw_images_horz(grids[0:5, :, :])\n",
    "\n",
    "        for pmap in self.maps:\n",
    "            draw_images_horz(pmap[:, :, :, 0])\n",
    "        \n",
    "    self.dataset.visualize(deX, est, deY)\n",
    "\n",
    "    self.need_maps = False\n",
    "    self.maps = None\n",
    "\n",
    "CnnBasicModel.visualize = cnn_basic_visualize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
